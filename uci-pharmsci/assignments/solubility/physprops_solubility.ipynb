{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1NzTCr7lFh2"
      },
      "source": [
        "# Solubility calculation assignment, PharmSci 175/275\n",
        "\n",
        "Solubility estimation/prediction is a huge problem in drug discovery. Here, we will attempt to build a simple empirical model for solubility prediction as in a recent literature challenge. We will take a set of ~100 solubility values, and develop a simple model which reproduces those values reasonably well, then test this model on a new set of compounds (a test set). To put it another way, we have a test set and a training set, and want to use the known solubilities from the training set to predict solubilities for the test set. \n",
        "\n",
        "This builds on the solubility challenge of [Llinàs et al.](https://dx.doi.org/10.1021/ci800058v) and the conclusions/subsequent work of [Hopfinger et al.](https://dx.doi.org/10.1021/ci800436c).\n",
        "\n",
        "\n",
        "## Overview\n",
        "\n",
        "Solubility calculation is an important problem for drug discovery, partly because it is so important that drugs be soluble. Solubility is an important factor in the design of orally bioavailable drugs, as we have discussed in class. However, no good physical models are available for work in this area yet, so most of the models for solubility estimation are empirical, based on measuring a set of simple molecular properties for molecules and combining these to estimate a solubility in some way, based on calibration to experimental data.\n",
        "\n",
        "Recently, Llinàs et al., [(J. Chem. Inf. Model 48:1289 (2008))](https://dx.doi.org/10.1021/ci800058v) posed a challenge: Can you predict a set of 32 solubilities on a test set, using a database (training set) of 100 reliable solubility measurements? Follow up work [(Hopfinger et al., J. Chem. Inf. Model 49:1 (2009))](https://dx.doi.org/10.1021/ci800436c) provided the solubility measurements of the test set and assessed performance of a wide variety of solubility estimation techniques in this challenge.\n",
        "\n",
        "Here, your job is to construct several simple linear models to predict solubilities using the training set of roughly 100 compounds, and then test their performance on the test set, comparing them with one another, with a null model, and with the performance of research groups which participated in the challenge. You should also implement and test a simple variant of the LINGO-based approach of Vidal et al. (J. Chem. Inf. Model 45(2):386-393 (2005)). \n",
        "\n",
        "A good deal of the technology you will need to use here is provided for you, including example models. Your job in this assignment is simply going to be to adjust the Python code I have provided to build several (five or more) new models for predicting solubilities, plus one based on the approach of Vidal, and compare their performance to select your favorite. \n",
        "\n",
        "## Some setup notes\n",
        "\n",
        "In this directory, you should also find a module you can import which will help with some statistics -- `tools.py`. You will also find two directories containing structures of molecules in the different sets -- `llinas_predict`, containing molecules whose solubilities we want to predict, and `llinas_set`, containing molecules in the training set. Additionally, in the `scripts` directory there is `solubilities.pickle` which contains solubility data (not human readable).  \n",
        "\n",
        "I also provide some fairly extensive example code below which you can use as the basis for your assignment. To briefly summmarize the provided code (you can see more detail by reading the comments and code below) it loads the structures of the molecules and their names, computes a reasonably extensive set of descriptros or properties of the different molecules and loads in the actual solubility data. It then proceeds to build two extremely simple models for predicting solubilities based on a simple linear combination/fit of physical properties. You will be able to use this part of the program as a template for building your own solubility models.\n",
        "\n",
        "## For solubility prediction, we'll use a series of *descriptors*\n",
        "\n",
        "Descriptors are properties of our molecule which might (or might not) be related to the solubility. For example, we might think that solubility will in general tend to go down as molecular weight goes up, and go up as polarity increases (or go down as polarity decreases) and so on. \n",
        "\n",
        "Here, let's take a sample molecule and calculate a series of descriptors which we might want to use in constructing a simple solubility model. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run cell if using collab\n",
        "\n",
        "# Import condacolab python library and install condacolab (~5 minutes). \n",
        "# Rerun cell after crashing\n",
        "!pip install --target=$nb_path -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "\n",
        "#check condacolab to ensure that it works\n",
        "condacolab.check()\n",
        "\n",
        "#other installs\n",
        "#!conda install -c conda-forge nb_conda nglview py3dmol mdtraj --yes \n",
        "!conda install -c anaconda scipy numpy --yes\n",
        "!conda install -c openeye openeye-toolkits --yes\n",
        "#!pip install --extra-index-url https://pypi.org/simple --extra-index-url https://pypi.anaconda.org/openeye/simple/ -i https://pypi.anaconda.org/openeye/label/oenotebook/simple openeye-oenotebook\n",
        "\n",
        "# Mount google drive to Colab Notebooks to access files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)\n",
        "\n",
        "# Move into directory \n",
        "%cd /content/drive/MyDrive/drug-computing/uci-pharmsci/assignments/solubility\n",
        "\n",
        "#set the OE_LICENSE environment variable to point to the license file\n",
        "%env OE_LICENSE=/content/drive/MyDrive/drug-computing/oelicense/oe_license.txt\n",
        "# Check the OE_LICENSE environment variable set\n",
        "%env"
      ],
      "metadata": {
        "id": "0bwMzlhElZAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run cell if using collab\n",
        "\n",
        "# Link openeye license to .bash_profile  \n",
        "%%shell \n",
        "echo export OE_LICENSE=\"/content/drive/MyDrive/drug-computing/oelicense/oe_license.txt\" >> ~/.bash_profile\n",
        "source ~/.bash_profile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e610ZtSl3HP",
        "outputId": "006ad2fa-57fb-4cd5-e6bd-7eae27e0a152"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AX7kfANzlFh-"
      },
      "outputs": [],
      "source": [
        "from openeye.oechem import *\n",
        "from openeye.oemolprop import *\n",
        "from openeye.oeiupac import *\n",
        "from openeye.oezap import *\n",
        "from openeye.oeomega import *\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "\n",
        "#Initialize an OpenEye molecule\n",
        "mol = OEMol()\n",
        "\n",
        "#let's look at phenol\n",
        "OEParseIUPACName( mol, 'naphthalene' )\n",
        "\n",
        "#Generate conformation\n",
        "omega = OEOmega()\n",
        "omega(mol)\n",
        "\n",
        "#Here one of the descriptors we'll use is the calculated solvation free energy, from OpenEye's ZAP electrostatics solver\n",
        "#Get zap ready for electrostatics calculations\n",
        "zap = OEZap()\n",
        "zap.SetInnerDielectric( 1.0 )\n",
        "zap.SetGridSpacing(0.5)\n",
        "area = OEArea()\n",
        "\n",
        "#Reduce verbosity\n",
        "OEThrow.SetLevel(OEErrorLevel_Warning)\n",
        "\n",
        "\n",
        "#Let's print a bunch of properties\n",
        "#Molecular weight\n",
        "print( \"Molecular weight: %.2f\" % OECalculateMolecularWeight(mol) )\n",
        "#Number of atoms\n",
        "print( \"Number of atoms: %s\" % mol.NumAtoms() ) \n",
        "#Number of heavy atoms\n",
        "print( \"Number of heavy atoms: %s\" % OECount(mol, OEIsHeavy() ) )\n",
        "#Number of ring atoms\n",
        "print( \"Number of ring atoms: %s\" % OECount(mol, OEAtomIsInRing() ) )\n",
        "#Number of halogens\n",
        "print( \"Number of halogens: %s\" % OECount( mol, OEIsHalogen() ))\n",
        "print (\"Number of nitrogens: %s\" % OECount( mol, OEIsNitrogen() ) )\n",
        "print( \"Number of oxygens: %s\" % OECount( mol, OEIsOxygen() ) )\n",
        "print( \"Number of rotatable bonds: %s\" % OECount( mol, OEIsRotor() ) )\n",
        "\n",
        "#Calculated logP - water to octanol partitioning coefficient (which is often something which may correlate somewhat with solubility)\n",
        "print( \"Calculated logP: %.2f\" %  OEGetXLogP( mol ) )\n",
        "\n",
        "print( \"Number of aromatic rings: %s\" % OEGetAromaticRingCount( mol ) )\n",
        "\n",
        "    \n",
        "    \n",
        "#Calculate lots of other properties using molprop toolkit as per example in OE MolProp manual\n",
        "#Handle the setup of 'filter', which computes lots of properties with the goal of filtering compounds. Here we'll not do any filtering\n",
        "#and will use it solely for property calculation\n",
        "filt = OEFilter()\n",
        "ostr = oeosstream()\n",
        "pwnd = False\n",
        "filt.SetTable( ostr, pwnd)\n",
        "headers = ostr.str().decode().split('\\t')\n",
        "ostr.clear()\n",
        "filt(mol)\n",
        "fields = ostr.str().decode().split('\\t')\n",
        "tmpdct = dict( zip(headers, fields) ) #Format the data we need into a dictionary for easy extraction\n",
        "\n",
        "print(\"Polar surface area: %s\" % tmpdct[ '2d PSA' ] )\n",
        "print(\"Number of hbond donors: %s\" % int(tmpdct['hydrogen-bond donors']) )\n",
        "print(\"Number of hbond acceptors: %s\" % int(tmpdct['hydrogen-bond acceptors']) )\n",
        "print (\"Number of rings: %s\" % int(tmpdct['number of ring systems']) )\n",
        "#print(tmpdct.keys())\n",
        "\n",
        "#Quickly estimate hydration free energy, or a value correlated with that -- from ZAP manual\n",
        "#Do ZAP setup for molecule\n",
        "OEAssignBondiVdWRadii(mol)\n",
        "OEMMFFAtomTypes(mol)\n",
        "OEMMFF94PartialCharges(mol)\n",
        "zap.SetMolecule( mol )\n",
        "solv = zap.CalcSolvationEnergy()\n",
        "aval = area.GetArea( mol )\n",
        "#Empirically estimate solvation free energy (hydration)\n",
        "solvation = 0.59*solv + 0.01*aval #Convert electrostatic part to kcal/mol; use empirically determined kcal/sq angstrom value times surface area term\n",
        "print (\"Calculated solvation free energy: %.2f\" % solvation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHnZZilJlFiC"
      },
      "source": [
        "## Linear models for solubility: Understanding your task\n",
        "\n",
        "Here, your first job is to construct some linear models for solubility and attempt to use them to predict solubilities for a test set of molecules. \n",
        "Many different models for solubilities would be possible. Here, however, we focus on linear models -- that is, models having the form:\n",
        "$y = mx + b$\n",
        "\n",
        "where $y$ is the solubility, $m$ and $b$ are constants, and $x$ is some descriptor of the molecule. Or with two variables:\n",
        "$y = mx + nz + b$\n",
        "\n",
        "Here we've added a second descrptor, $z$, and another constant, $n$. Still more generally, we could write:\n",
        "\n",
        "$y = b + \\sum_i m_i x_i$\n",
        "\n",
        "In this case, we now have a constant, $b$, and a set of other constants, $m_i$, and descriptors, $x_i$; the sum runs over all values of $i$.\n",
        "\n",
        "What does this all mean? Basically, we are going to assume that we can predict solubilities out of some linear combination of descriptors or molecular properties. For example, (as a null model) I might assume that solubility can be predicted simply based on molecular weight -- perhaps heavier compounds will in general be less (or more) soluble. I might write:\n",
        "\n",
        "$y = m\\times MW + b$\n",
        "\n",
        "This has the form $y=mx + b$ but I replaced $x$ with $MW$, the molecular weight. To fit this model, I would then need to find the coefficients $m$ and $b$ to give the best agreement with the actual solublity data.\n",
        "\n",
        "Here, I would first develop parameters $m$ and $b$ to fit my training set -- that is, I would fit $m$ and $b$ on the training set data, the (roughly 100) compounds provided in the first paper. Then, I would apply the same $m$ and $b$ to the test set data to see how well I can predict the 32 \"new\" compounds. \n",
        "\n",
        "In this project, you will test the \"null model\" I just described (which turns out actually to be not too bad, here!), as well as another model I built, which has the form\n",
        "\n",
        "$y = m\\times MW +n\\times F + b$\n",
        "where I've added a new descriptor, F, which is the calculated hydration free energy of the compound (calculated with a PB model). So my model predicts that solubility is a constant plus some factor times molecular weight and another factor times the calculated hydration free energy.\n",
        "\n",
        "Finding the parameters $m$, $n$, and $b$ is a very simple via a least-squares fit. This is done for you within Python. \n",
        "Here you will need to develop several of your own linear solubility prediction models (as discussed below) and test their performance.\n",
        "\n",
        "## Lingo-based solubility models\n",
        "\n",
        "In class, when we discussed the LINGO similarity approach, I mentioned in passing that this approach had been used to attempt to estimate solubilities based on functional group/LINGO fragment contributions. This was done in work by Vidal et al. (J. Chem. Inf. Model 45(2):386-393 (2005)).\n",
        "\n",
        "While the approach of Vidal et al. is outside the scope of this assignment, you should quickly implement a related idea (optional for undergraduates). Particularly, you should test what happens if, for each compound in your test set you simply predict the value of the most similar (by LINGO) compound in the training set. This will allow you to quickly test how well you can predict solubilities based on pure molecular similarity to compounds in your training set. Obviously your training set is limited in size, but it’s still a worthwhile test.\n",
        "\n",
        "## Your assignment: Build and test at least five new solubility models plus (for graduate students) the Lingo-based approach\n",
        "\n",
        "This section deals with what you are trying to do. A separate section, below, deals with the “how to” aspect. Your goal in this project is to build and test five new solubility models plus the approach based on LINGO similarity. \n",
        "\n",
        "This section focuses primarily on building linear solubility models; I’ll assume the LINGO similarity idea is simple enough you can implement it yourself. (Though if you like,  for extra credit, you can combine it with a linear solubility model to see if it can do better than either approach alone.)\n",
        "\n",
        "**Building solubility models**: Building a solubility model, here, amounts to selecting a set of descriptors (possible choices are listed below), getting their values, and then doing a least squares fit on the training set (the knowns) to find the parameters.\n",
        "\n",
        "**Testing solubility models**: Testing a solubility model, here, means taking the parameters that were found for a specific solubility model and applying that model to the test set, predicting solubility values and seeing how well the predicted values compare to experiment. \n",
        "\n",
        "**Descriptors**: Here, a variety of molecular descriptors are precalculated for you. These are stored below within a dictionary, `compounds`, such that `compounds[molname][descriptorname]` gives you the value of the descriptor named `descriptorname` for compound name `molname`. For example, `compounds['naloxone']['mw']` gives the molecular weight of naloxone. Here are the descriptors available to you below, by their abbreviation (i.e. \"mw\" for molecular weight) with a small amount of information about each:\n",
        "- `mw`: Molecular weight\n",
        "- `numatoms`: Number of atoms including hydrogens\n",
        "- `heavyatoms`: Number of heavy atoms\n",
        "- `ringatoms`: Number of atoms in rings\n",
        "- `halogens`: Number of halogens\n",
        "- `nitrogens`: Number of nitrogens\n",
        "- `oxygens`: Number of oxygens\n",
        "- `rotatable`: Number of rotatable bonds\n",
        "- `XlogP`: Calculated logP (water to octanol partitioning coefficient)\n",
        "- `aromaticrings`: Number of aromatic rings\n",
        "- `PSA`: Polar surface area of the compound\n",
        "- `SA`: Surface area of the compound\n",
        "- `hbond-donors`: Number of hydrogen bond donors\n",
        "- `hbond-acceptors`: Number of hydrogen bond acceptors\n",
        "- `rings`: Number of rings \n",
        "- `hydration`: Estimated hydration free energy (essentially a measure of the interactions with solvent)\n",
        "\n",
        "\n",
        "As you might guess, some of these probably ought to have more to do with solubility than others. The number of atoms in rings is, perhaps, not that related to solubility, nor should the number of rings be that related to solubility. Perhaps there may generally be a trend that larger compounds are somewhat less soluble -- not for chemical reasons, but rather for reasons of pharmaceutical interest (many drugs tend to be somewhat large and somewhat less soluble), so some of the descriptors correlated with molecular weight (such as number of atoms, number of heavy atoms, etc.) may be better predictors of solubility than you might guess. On the other hand, hydration free energy is closely related to solubility (it’s the solution part of a solubility), and some of the other descriptors may be as well.\n",
        "\n",
        "In any case, one of the goals here is to build a variety of different models to start seeing (a) how you typically can get better results in the training set as you keep adding more descriptors; (b) which descriptors tend to work better; and (c) how well your best model(s) can do on the test set. You may also gain some insight into (d), how to avoid overfitting. \n",
        "\n",
        "So, overall, you should select some specific descriptors you think are interesting, and build models involving those. Be sure to also test the approach based on LINGO similarity if you are a grad student (you will have to implement it based on the LINGO examples already seen earlier in the course).\n",
        "\n",
        "### Solubility versus log S\n",
        "\n",
        "Solubilities potentially cover a huge range. In fact, this dataset tends to have a relatively large number of compounds which are not very soluble, and a small number which are extremely soluble. What this means is that if we aren’t careful, the few extremely soluble compounds will end up playing a huge role in the development of our models. Thus, here, it actually makes more sense to work with the logarithm of the solubility, which we’ll call logS. So, in our project, our real goal is going to be to calculate the logS, not the solubility itself. My code has been written to work with logS, so henceforth when I talk about solubility I’m really going to be talking about logS.\n",
        "\n",
        "### How to achieve your goal: Some specific hints\n",
        "\n",
        "To get going on the problem, view the code below and find the section below dealing with building first and second simple models. Here, I provide two initial models noted above -- one based on molecular weight as a descriptor, and one based on molecular weight plus hydration free energy. For your starting point, read through the code for [\"Build a first simple model\"](#Build-a-first-simple-model) based on hydration free energy and molecular weight. You will basically need to copy this code and modify it to handle your descriptors.\n",
        "\n",
        "Take a quick look for the code for [\"Build a second simple model\"](#Build-another-simple-model). There, the first step, before we can build a model, is to get values of our descriptors for the molecules of interest. We’ve already done that for the molecular weight in [\"Build a first simple model\"](#Build-a-first-simple-model) (refer there if you like), so this code begins by getting the hydration free energies for the knowns and the molecules we want to predict. The code is commented, but basically what you need to know is that if you want to switch to another metric, say, number of rings, you’d take the code like\n",
        "\n",
        "```python\n",
        "known_hydr = [ compounds[mol]['hydration'] for mol in knownnames ] \n",
        "known_hydr = np.array(known_hydr)\n",
        "```\n",
        "and switch it to\n",
        "```python\n",
        "known_rings = [ compounds[mol]['rings'] for mol in knownnames ] \n",
        "known_rings = np.array(known_rings)\n",
        "```\n",
        "\n",
        "This gets descriptor values for the number of rings for the knowns (training set molecules). You’d then need to do the same for changing `p_hydr` into `p_rings` (number of rings for the \"prediction\" or test set molecules).\n",
        "\n",
        "Then, in the next section, there least squares fit is done to actually get the parameters. The formatting here is a little tricky, but the main thing you need to know is that this code\n",
        "\n",
        "```python\n",
        "A = np.vstack( [known_mw, known_hydr, np.ones(len(known_mw) ) ] ).T\n",
        "```\n",
        "provides your descriptors in a list, followed by `np.ones...`. So if you wanted to switch this to use rings, molecular weight, and hydration, you'd do something like:\n",
        "```python\n",
        "A = np.vstack( [known_rings, known_mw, known_hydr, np.ones(len(known_mw) ) ] ).T\n",
        "```\n",
        "\n",
        "The actual least-squares fit is done by this:\n",
        "```python\n",
        "m, n, b = np.linalg.lstsq( A, known_solubilities)[0]\n",
        "```\n",
        "\n",
        "For the case where you'd fitted rings, molecular weight, and hydration, you would calculate the resulting fitted values using:\n",
        "```python\n",
        "m, n, o, b = np.linalg.lstsq( A, known_solubilities)[0]\n",
        "fittedvals = m*known_rings + n*known_mw + o*known_hydr + b\n",
        "```\n",
        "\n",
        "You'd make similar changes to the computation of `predictvals` to parallel those made calculating `fittedvals`. You can leave all of the statistics code below that unchanged, and just modify the print statements to indicate what model it is you are testing. \n",
        "\n",
        "**Be sure to read the discussion below before getting too carried away on the problem**, as it provides some more information on assessing what is and what isn’t a good model.\n",
        "\n",
        "\n",
        "### Performance metrics for your models\n",
        "\n",
        "As noted in class, one should always have metrics for judging the performance of a model. Here, my code (in `tools.py`, imported below) provides several. The Kendall tau value is a measure of ability to rank-order pairs of compounds, and runs from -1 (every pair ranked in the opposite order) to 1 (every pair ranked perfectly) with a value of 0 corresponding to every pair being ranked incorrectly. The RMS error measures a type of average error across the entire set of compounds relative to experiment; units here are logS, and lower values mean lower error on average. The $R^2$ (here called `R2` or `Rsquared`) value is the correlation coefficient, and like the Kendall tau has to do with predictive power (in this case, how well the calculated values correlate with the experimental ones), though it has some limitations (such as sensitivity to extremes of the data). It runs from -1 to 1, with -1 meaning perfect anticorrelation, 1 meaning perfect correlation, and 0 meaning no correlation. Also, for the purposes of comparison with the Hopfinger paper, I have provided code to calculate the percentage of predictions within 0.5 log units, which will allow you to compare with the different methods listed there in terms of both RMS error and percentage correct. \n",
        "\n",
        "In addition to these metrics, the code also automatically compares to the null hypothesis that there is no correlation between the calculated and measured logS values, and provides the probability (based on the Kendall tau test) that you could get this Kendall tau accidentally when in fact there was no correlation. When this probability is extremely small, it means that your model almost certainly has at least some predictive power.\n",
        "\n",
        "In general, what you should see is that as you make your models better, the Kendall tau and $R^2$ values should go up towards 1, and the RMS error should go down. You should also see the probability of getting the Kendall tau value by change go down towards zero. \n",
        "\n",
        "### Reminder concerning good versus bad models\n",
        "\n",
        "Remember that, as discussed in class, adding parameters to a model should always make it fit the data better. That is to say, if you compare to models, one using one descriptor, and another using two descriptors, in general the model with two descriptors should have a higher Kendall tau on the training set and a lower RMS error than the model with one descriptor. This doesn’t mean the model with two descriptors is better, necessarily -- it just means it has more parameters.\n",
        "\n",
        "So, as we noted in class, a good model is, in general, the simplest possible model that fits the data well enough. And a model with fewer parameters is generally preferable over one with more. Also, a good model should perform relatively similarly on the training set (the known compounds) versus the test set (those we are predicting). So, as you construct your models, you may want to keep this in mind. It also might be worth deliberately trying to construct a model which is overfitted, perhaps by including a whole lot of descriptors, until you reach the point where your performance is significantly worse in the test set than the training set.\n",
        "\n",
        "### Statistical significance tests\n",
        "\n",
        "In general, we should also be calculating uncertainties for our different metrics, and applying statistical significance tests to test whether each new model is significantly different than the old model. For example, the t-test could be used to attempt to reject the null hypothesis that a new model is no better on average than the old model. Also, having error bars (calculated via bootstrapping, for example) on the RMS error, $R^2$, etc., could also help us know when two models are not significantly different. However, because this assignment must be done fairly quickly, these tests are not included as part of it.\n",
        "\n",
        "### What to do and what to turn in\n",
        "\n",
        "You need to build and test at least five different models. You should try at least one model that uses four or more descriptors, hopefully getting to the point where you see significantly worse performance on the test set than on the training set. Keep track of every set of descriptors you try. \n",
        "\n",
        "When you complete the assignment, turn in a brief report (entering it below following the code is fine) containing your discussion and any relevant statistics, etc. This should include:\n",
        "- Your Python code \n",
        "- The sets of descriptors you tried\n",
        "- The statistics describing performance of the model you believe is best, and a brief description of why you chose that model as best\n",
        "- A brief discussion comparing your best model with performance of the contestants in Hopfinger et al., as per the logS section of table 2 on the 28 compound test. Specifically, you should be able to compare your Rsquared and percentage within 0.5 log units with the values given in that table. Is your simple model beating many of the contestants? Why do you think that is? How much worse is it than the best models? \n",
        "- (If you did the LINGO section -- mandatory for graduate students) Comment on how well the LINGO similarity approach worked relative to other approaches you tried, and why you think it succeeded or failed.\n",
        "\n",
        "\n",
        "# Now here's the material you need to get going\n",
        "\n",
        "Here's the Python code I'm providing for you which will form the starting point for your assignment.\n",
        "\n",
        "## Get some things set up"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "#IMPORTS OF PACKAGES NEEDED\n",
        "#============================================================================\n",
        "import tools\n",
        "import pickle\n",
        "from openeye.oechem import *\n",
        "from openeye.oemolprop import *\n",
        "from openeye.oezap import *\n",
        "import glob\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "%pylab inline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVyMFt8YnuVZ",
        "outputId": "6b01094e-6b0c-4b5b-a43b-5f224f960cf2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['Text', 'Arrow']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pUx4vuHClFiG"
      },
      "outputs": [],
      "source": [
        "#============================================================================\n",
        "#LOAD OUR MOLECULES FOR WHICH WE ARE PREDICTING SOLUBILITIES\n",
        "#============================================================================\n",
        "\n",
        "#Load our molecules, storing lists of the names of the knowns and the ones to predict, and storing the actual molecules to a dictionary.\n",
        "molecules = glob.glob('llinas_set/*.sdf')\n",
        "molecules = molecules + glob.glob('llinas_predict/*.sdf')\n",
        "compounds = {}\n",
        "knownnames = [] #This will be a list of the molecules in our training set -- molecules with 'known' solubilities\n",
        "predictnames = [] #This will be a list of molecules in the test set -- molecules with solubilities we are trying to 'predict'\n",
        "\n",
        "#Loop over molecules and load files, storing them to a 'compounds' dictionary\n",
        "for filename in molecules:\n",
        "    name = filename.split('/')[1].replace('.sdf','')\n",
        "    compounds[name] = {}\n",
        "    istream = oemolistream(filename)\n",
        "    mol = OEMol()\n",
        "    OEReadMolecule( istream, mol )\n",
        "    compounds[name]['mol'] = mol\n",
        "    istream.close()\n",
        "    if 'predict' in filename:\n",
        "        predictnames.append(name)\n",
        "    else:\n",
        "        knownnames.append(name)\n",
        "        \n",
        "#Make a list of all the molecule names\n",
        "molnames = knownnames + predictnames\n",
        "\n",
        "#============================================================================\n",
        "#MISCELLANEOUS PREP\n",
        "#============================================================================\n",
        "\n",
        "#Get zap ready for electrostatics calculations\n",
        "zap = OEZap()\n",
        "zap.SetInnerDielectric( 1.0 )\n",
        "zap.SetGridSpacing(0.5)\n",
        "area = OEArea()\n",
        "\n",
        "#Reduce verbosity\n",
        "OEThrow.SetLevel(OEErrorLevel_Warning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMx6B4UvlFiI"
      },
      "source": [
        "## Compute some descriptors and store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXbAHdRflFiI",
        "outputId": "cbeaa2b3-55ba-4701-cef5-35034a868aee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating descriptors for 4-hydroxybenzoicacid (1/123)...\n",
            "Calculating descriptors for bromogramine (2/123)...\n",
            "Calculating descriptors for carprofen (3/123)...\n",
            "Calculating descriptors for guanine (4/123)...\n",
            "Calculating descriptors for ranitidine (5/123)...\n",
            "Calculating descriptors for oxytetracycline (6/123)...\n",
            "Calculating descriptors for pindolol (7/123)...\n",
            "Calculating descriptors for maprotiline (8/123)...\n",
            "Calculating descriptors for sarafloxacin (9/123)...\n",
            "Calculating descriptors for mefenamicacid (10/123)...\n",
            "Calculating descriptors for acetazolamide (11/123)...\n",
            "Calculating descriptors for nitrofurantoin (12/123)...\n",
            "Calculating descriptors for nalidixicacid (13/123)...\n",
            "Calculating descriptors for warfarin (14/123)...\n",
            "Calculating descriptors for chlorprothixene (15/123)...\n",
            "Calculating descriptors for sulindac (16/123)...\n",
            "Calculating descriptors for alprenolol (17/123)...\n",
            "Calculating descriptors for trimipramine (18/123)...\n",
            "Calculating descriptors for quinine (19/123)...\n",
            "Calculating descriptors for verapamil (20/123)...\n",
            "Calculating descriptors for chlorpromazine (21/123)...\n",
            "Calculating descriptors for flurbiprofen (22/123)...\n",
            "Calculating descriptors for flumequine (23/123)...\n",
            "Calculating descriptors for ciprofloxacin (24/123)...\n",
            "Calculating descriptors for diclofenac (25/123)...\n",
            "Calculating descriptors for phenazopyridine (26/123)...\n",
            "Calculating descriptors for 55-diphenylhydantoin (27/123)...\n",
            "Calculating descriptors for amiodarone (28/123)...\n",
            "Calculating descriptors for ibuprofen (29/123)...\n",
            "Calculating descriptors for meclizine (30/123)...\n",
            "Calculating descriptors for metronidazole (31/123)...\n",
            "Calculating descriptors for sulfamethazine (32/123)...\n",
            "Calculating descriptors for hexobarbital (33/123)...\n",
            "Calculating descriptors for sertraline (34/123)...\n",
            "Calculating descriptors for thymol (35/123)...\n",
            "Calculating descriptors for lomefloxacin (36/123)...\n",
            "Calculating descriptors for l-proline (37/123)...\n",
            "Calculating descriptors for phenobarbital (38/123)...\n",
            "Calculating descriptors for diltiazem (39/123)...\n",
            "Calculating descriptors for niflumicacid (40/123)...\n",
            "Calculating descriptors for procaine (41/123)...\n",
            "Calculating descriptors for norfloxacin (42/123)...\n",
            "Calculating descriptors for fenoprofen (43/123)...\n",
            "Calculating descriptors for 5-fluorouracil (44/123)...\n",
            "Calculating descriptors for diphenhydramine (45/123)...\n",
            "Calculating descriptors for flufenamicacid (46/123)...\n",
            "Calculating descriptors for phenylbutazone (47/123)...\n",
            "Calculating descriptors for tryptamine (48/123)...\n",
            "Calculating descriptors for difloxacin (49/123)...\n",
            "Calculating descriptors for 2-amino-3-bromobenzoicacid (50/123)...\n",
            "Calculating descriptors for procainamide (51/123)...\n",
            "Calculating descriptors for trichlormethiazide (52/123)...\n",
            "Calculating descriptors for glipizide (53/123)...\n",
            "Calculating descriptors for tetracycline (54/123)...\n",
            "Calculating descriptors for chlorpropamide (55/123)...\n",
            "Calculating descriptors for chlorpheniramine (56/123)...\n",
            "Calculating descriptors for orbifloxacin (57/123)...\n",
            "Calculating descriptors for sparfloxacin (58/123)...\n",
            "Calculating descriptors for 1-naphthol (59/123)...\n",
            "Calculating descriptors for 1-benzylimidazole (60/123)...\n",
            "Calculating descriptors for sulfathiazole (61/123)...\n",
            "Calculating descriptors for 4-iodophenol (62/123)...\n",
            "Calculating descriptors for ofloxacin (63/123)...\n",
            "Calculating descriptors for enrofloxacin (64/123)...\n",
            "Calculating descriptors for metoclopramide (65/123)...\n",
            "Calculating descriptors for nortriptyline (66/123)...\n",
            "Calculating descriptors for phthalicacid (67/123)...\n",
            "Calculating descriptors for micronazole (68/123)...\n",
            "Calculating descriptors for amodiaquine (69/123)...\n",
            "Calculating descriptors for danofloxacin (70/123)...\n",
            "Calculating descriptors for tolmetin (71/123)...\n",
            "Calculating descriptors for naloxone (72/123)...\n",
            "Calculating descriptors for amitriptyline (73/123)...\n",
            "Calculating descriptors for 110-phenanthroline (74/123)...\n",
            "Calculating descriptors for 5-bromo-24-dihydroxy-benzoicacid (75/123)...\n",
            "Calculating descriptors for azathioprine (76/123)...\n",
            "Calculating descriptors for tetracaine (77/123)...\n",
            "Calculating descriptors for carvedilol (78/123)...\n",
            "Calculating descriptors for propranolol (79/123)...\n",
            "Calculating descriptors for amantadine (80/123)...\n",
            "Calculating descriptors for bupivacaine (81/123)...\n",
            "Calculating descriptors for atropine (82/123)...\n",
            "Calculating descriptors for hydroflumethiazide (83/123)...\n",
            "Calculating descriptors for naproxen (84/123)...\n",
            "Calculating descriptors for papaverine (85/123)...\n",
            "Calculating descriptors for cimetidine (86/123)...\n",
            "Calculating descriptors for piroxicam (87/123)...\n",
            "Calculating descriptors for loperamide (88/123)...\n",
            "Calculating descriptors for levofloxacin (89/123)...\n",
            "Calculating descriptors for trimethoprim (90/123)...\n",
            "Calculating descriptors for desipramine (91/123)...\n",
            "Calculating descriptors for cephalothin (92/123)...\n",
            "Calculating descriptors for famotidine (93/123)...\n",
            "Calculating descriptors for sulfacetamide (94/123)...\n",
            "Calculating descriptors for diazoxide (95/123)...\n",
            "Calculating descriptors for deprenyl (96/123)...\n",
            "Calculating descriptors for acetaminophen (97/123)...\n",
            "Calculating descriptors for amoxicillin (98/123)...\n",
            "Calculating descriptors for clozapine (99/123)...\n",
            "Calculating descriptors for acebutolol (100/123)...\n",
            "Calculating descriptors for ketoprofen (101/123)...\n",
            "Calculating descriptors for sulfamerazine (102/123)...\n",
            "Calculating descriptors for diethylstilbestrol (103/123)...\n",
            "Calculating descriptors for tolbutamide (104/123)...\n",
            "Calculating descriptors for bendroflumethiazide (105/123)...\n",
            "Calculating descriptors for trazodone (106/123)...\n",
            "Calculating descriptors for indomethacin (107/123)...\n",
            "Calculating descriptors for furosemide (108/123)...\n",
            "Calculating descriptors for meclofenamicacid (109/123)...\n",
            "Calculating descriptors for 1-napthoicacid (110/123)...\n",
            "Calculating descriptors for folicacid (111/123)...\n",
            "Calculating descriptors for hydrochlorothiazide (112/123)...\n",
            "Calculating descriptors for dipyridamole (113/123)...\n",
            "Calculating descriptors for terfenadine (114/123)...\n",
            "Calculating descriptors for lidocaine (115/123)...\n",
            "Calculating descriptors for imipramine (116/123)...\n",
            "Calculating descriptors for thiabendazole (117/123)...\n",
            "Calculating descriptors for benzocaine (118/123)...\n",
            "Calculating descriptors for dibucaine (119/123)...\n",
            "Calculating descriptors for pyrimethamine (120/123)...\n",
            "Calculating descriptors for probenecid (121/123)...\n",
            "Calculating descriptors for diflunisal (122/123)...\n",
            "Calculating descriptors for salicylicacid (123/123)...\n"
          ]
        }
      ],
      "source": [
        "#============================================================================\n",
        "#COMPUTE DESCRIPTORS FOR OUR MOLECULES -- VARIOUS PROPERTIES OF THE MOLECULES WHICH MIGHT BE USEFUL IN SOLUBILITY ESTIMATION\n",
        "#============================================================================\n",
        "\n",
        "#Compute a bunch of descriptors for our molecules. Descriptors will be stored in the compounds dictionary, by compound name.\n",
        "#For example, compounds['terfenadine']['mw'] will give the 'mw' (molecular weight) of terfenadine).\n",
        "#A full description of the descriptors calculated will be put in the homework writeup.\n",
        "\n",
        "#Loop over molecules\n",
        "for molname in molnames:\n",
        "    print(\"Calculating descriptors for %s (%s/%s)...\" % (molname, molnames.index(molname)+1, len(molnames) )) #Print progress\n",
        "\n",
        "    #Load the OEMol representation of our molecule from where it's stored\n",
        "    mol = compounds[molname]['mol']\n",
        "\n",
        "    #Compute molecular weight and store\n",
        "    compounds[ molname ]['mw'] = OECalculateMolecularWeight( mol )\n",
        "\n",
        "    #Number of atoms -- store\n",
        "    compounds[molname]['numatoms'] = mol.NumAtoms()\n",
        "\n",
        "    #Number of heavy atoms\n",
        "    compounds[molname]['heavyatoms'] = OECount(mol, OEIsHeavy() )\n",
        "\n",
        "    #Number of ring atoms\n",
        "    compounds[molname]['ringatoms'] = OECount(mol, OEAtomIsInRing() )\n",
        "\n",
        "    #Number of halogens\n",
        "    compounds[molname]['halogens'] = OECount( mol, OEIsHalogen() )\n",
        "\n",
        "    #Number of nitrogens\n",
        "    compounds[molname]['nitrogens'] = OECount( mol, OEIsNitrogen() )\n",
        "\n",
        "    #Number of oxygens\n",
        "    compounds[molname]['oxygens'] = OECount( mol, OEIsOxygen() )\n",
        "\n",
        "    #Number of rotatable bonds\n",
        "    compounds[molname]['rotatable'] = OECount( mol, OEIsRotor() )\n",
        "\n",
        "    #Calculated logP\n",
        "    compounds[molname]['XlogP'] = OEGetXLogP( mol )\n",
        "\n",
        "    #Number of aromatic rings\n",
        "    compounds[molname]['aromaticrings'] = OEGetAromaticRingCount( mol )\n",
        "\n",
        "    #Calculate lots of other properties using molprop toolkit as per example in OE MolProp manual\n",
        "    #Handle the setup of 'filter', which computes lots of properties with the goal of filtering compounds. Here we'll not do any filtering\n",
        "    #and will use it solely for property calculation\n",
        "    filt = OEFilter()\n",
        "    ostr = oeosstream()\n",
        "    pwnd = False\n",
        "    filt.SetTable( ostr, pwnd)\n",
        "    headers = ostr.str().decode('UTF-8').split('\\t')\n",
        "    ostr.clear()\n",
        "    filt(mol)\n",
        "    fields = ostr.str().decode('UTF-8').split('\\t')\n",
        "    tmpdct = dict( zip(headers, fields) ) #Format the data we need into a dictionary for easy extraction\n",
        "    \n",
        "    #Extract polar surface area, store\n",
        "    compounds[molname]['PSA'] = tmpdct[ '2d PSA' ]\n",
        "    #Number of hbond donors\n",
        "    compounds[molname]['hbond-donors'] = int(tmpdct['hydrogen-bond donors'])\n",
        "    #Number of hbond acceptors\n",
        "    compounds[molname]['hbond-acceptors'] = int(tmpdct['hydrogen-bond acceptors'])\n",
        "    #Number of rings\n",
        "    compounds[molname]['rings'] = int(tmpdct['number of ring systems'])\n",
        "\n",
        "    #Quickly estimate hydration free energy, or a value correlated with that -- from ZAP manual\n",
        "    #Do ZAP setup for molecule\n",
        "    OEAssignBondiVdWRadii(mol)\n",
        "    OEMMFFAtomTypes(mol)\n",
        "    OEMMFF94PartialCharges(mol)\n",
        "    zap.SetMolecule( mol )\n",
        "    solv = zap.CalcSolvationEnergy()\n",
        "    aval = area.GetArea( mol )\n",
        "    #Empirically estimate solvation free energy (hydration)\n",
        "    solvation = 0.59*solv + 0.01*aval #Convert electrostatic part to kcal/mol; use empirically determined kcal/sq angstrom value times surface area term\n",
        "    compounds[molname]['hydration'] = solvation\n",
        "    #Also store surface area\n",
        "    compounds[molname]['SA'] = aval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2y1-E4hlFiJ"
      },
      "source": [
        "## Load in the reference data from Llinas et al./Hopfinger et al."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "j0ee4UJSlFiK"
      },
      "outputs": [],
      "source": [
        "#============================================================================\n",
        "# LOAD AND PREP THE ACTUAL SOLUBILITY DATA WE'LL BE USING\n",
        "#============================================================================\n",
        "\n",
        "#Load solubility data\n",
        "import pickle\n",
        "file = open('scripts/solubilities.pickle', 'rb')\n",
        "solubilities = pickle.load(file)\n",
        "file.close()\n",
        "new_solubilities = {}\n",
        "#Adjust some naming to match that from file names\n",
        "for name in solubilities.keys():\n",
        "    newname = name.replace(',','').replace(' ','')\n",
        "    new_solubilities[newname] = solubilities[name]\n",
        "solubilities = new_solubilities\n",
        "        \n",
        "#Build arrays of solubilities -- actually, work with logarithms of solubilities since they cover such a huge range\n",
        "#Build a list of the solubilities for the molecules in the training set (knowns)\n",
        "known_solubilities = [ solubilities[mol] for mol in knownnames]\n",
        "#Convert to an array and take the log\n",
        "known_solubilities = log(np.array( known_solubilities)) #Note conversion to log\n",
        "#Build a list of the solubilities for molecules in the test set (unknowns)\n",
        "predict_solubilities = [ solubilities[mol] for mol in predictnames]\n",
        "#Convert to an array and take the log\n",
        "predict_solubilities = log(np.array( predict_solubilities )) #Note conversion to log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-3nrdtDlFiK"
      },
      "source": [
        "## Build a first simple model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f-D5g4FlFiL"
      },
      "outputs": [],
      "source": [
        "#============================================================================\n",
        "# BUILD SOME SAMPLE MODELS TO PREDICT SOLUBILITY\n",
        "#    You will want to read this code and make sure you get it, as your task takes off from here\n",
        "#============================================================================\n",
        "#SIMPLE MODEL #1: Predict solubility based on molecular weight alone\n",
        "#============================================================================\n",
        "\n",
        "\n",
        "#Build a really really simple model -- predict solubility based on molecular weight\n",
        "\n",
        "#To do this, start by obtaining molecular weights -- for both the knowns (training set) and unknowns (test set)\n",
        "#Make a list of molecular weight for the knowns, convert to array\n",
        "known_mw = [ compounds[mol]['mw'] for mol in knownnames ]\n",
        "known_mw = np.array(known_mw)\n",
        "#Make a list of molecular weights to predict (test set), convert to array\n",
        "p_mw = [compounds[mol]['mw'] for mol in predictnames ]\n",
        "p_mw = np.array(p_mw)\n",
        "\n",
        "#Our model will have the form (using y for logS, the log of the solubility), y = m*(mw) + b, which we rewrite (to feed into numpy) as y = A * p where A is an array consisting of [ mw, 1] and p is [m, b].\n",
        "A = np.vstack( [known_mw, np.ones( len(known_mw) )] ).T #Write the array -- first our x value, then a 1 for the constant term\n",
        "\n",
        "#Solve for coefficients using least squares fit -- we just put the array A and the thing we want to fit (known_solubilities) into the least squares algorithm and get back the coefficients m and b\n",
        "m, b = np.linalg.lstsq( A, known_solubilities)[0]\n",
        "print(\"Fit coefficients: %.2f, %.2f\" % (m, b))\n",
        "\n",
        "#Compute the calculated y values, y = m*x + b, for the test set\n",
        "fittedvals = m*known_mw + b\n",
        "\n",
        "#Compute some statistics for our model -- Kendall tau, RMS error, correlation coefficient\n",
        "ktau, pvalue = scipy.stats.kendalltau( known_solubilities, fittedvals)\n",
        "rms = tools.rmserr( known_solubilities, fittedvals)\n",
        "R2 = tools.correl( known_solubilities, fittedvals)**2\n",
        "print(\"For initial (molecular weight) model training, Kendall tau is %.2f, RMS error is %.2f, and Rsquared is %.2f. Probability of getting this Kendall tau value when in fact there is no correlation (null hypothesis): %.2g\" % (ktau, rms, R2, pvalue))\n",
        "\n",
        "#Now test its predictive power by applying it to the test set\n",
        "predictvals = m*p_mw + b\n",
        "ktau, pvalue = scipy.stats.kendalltau( predict_solubilities, predictvals)\n",
        "rms = tools.rmserr( predict_solubilities, predictvals)\n",
        "R2 = tools.correl( predict_solubilities, predictvals)**2\n",
        "halflog = tools.percent_within_half( predict_solubilities, predictvals ) #Figure out percentage within 0.5 log units\n",
        "print(\"For initial (molecular weight) model test, Kendall tau is %.2f, RMS error is %.2f, and Rsquared is %.2f. Probability of getting this Kendall tau value when in fact there is no correlation (null hypothesis): %.2g. Percentage within 0.5 log units: %.2f\" % (ktau, rms, R2, pvalue, halflog))\n",
        "\n",
        "#Now, for fun, take all of the data (training and test set) and do a plot of the actual values versus molecular weight (for test and training set separately) and then an overlay of the predicted fit\n",
        "plot( known_mw, known_solubilities, 'bo' ) #Plot knowns with blue circles\n",
        "plot( p_mw, predict_solubilities, 'rs' ) #Plot test set with red squares\n",
        "\n",
        "#Do a plot of the predicted fit\n",
        "#First, figure out molecular weight range\n",
        "minmw = min( known_mw.min(), p_mw.min() )\n",
        "maxmw = max( known_mw.max(), p_mw.max() )\n",
        "#Compute solubility estimates corresponding to the minimum and maximum\n",
        "minsol = m*minmw+b\n",
        "maxsol = m*maxmw+b\n",
        "#Plot a line\n",
        "plot( [ minmw, maxmw], [minsol, maxsol], 'k-' ) #Plot as a black line overlaid\n",
        "xlabel('Molecular weight')\n",
        "ylabel('logS')\n",
        "\n",
        "# Show figure\n",
        "show()\n",
        "\n",
        "#Save figure\n",
        "savefig('mw_model.pdf')\n",
        "#Clear\n",
        "figure()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zikvV0dlFiM"
      },
      "source": [
        "## Build another simple model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZX5x4TNlFiM"
      },
      "outputs": [],
      "source": [
        "#============================================================================\n",
        "#SIMPLE MODEL #2: Predict based on hydration free energy (ought to have something to do with solubility) plus molecular weight\n",
        "#============================================================================\n",
        "\n",
        "\n",
        "#Build another model -- this time using hydration free energy plus molecular weight (should do better on training set, not clear if it will on test set)\n",
        "print(\"\\nHydration plus mw model:\")\n",
        "known_hydr = [ compounds[mol]['hydration'] for mol in knownnames] #Build a list of hydration free energies for the knowns, with names listed in knownnames (that is, hydration free energies for the training set)\n",
        "known_hydr = np.array(known_hydr) #Convert this to a numpy array\n",
        "p_hydr = [ compounds[mol]['hydration'] for mol in predictnames] #Build list of hydration free energies for the test set\n",
        "p_hydr = np.array(p_hydr) #Convert to numpy array\n",
        "\n",
        "#Prep for least squares fit and perform it\n",
        "A = np.vstack( [known_mw, known_hydr, np.ones(len(known_mw) ) ] ).T #Write array for fit -- see more detailed discussion above in the molecular weight section\n",
        "#Solve for coefficients\n",
        "m, n, b = np.linalg.lstsq( A, known_solubilities)[0]\n",
        "print(\"Fit coefficients: %.2f (mw), %.2f (hyd), %.2f (constant)\" % (m, n, b))\n",
        "fittedvals = m*known_mw + n*known_hydr + b #Calculate the values we 'predict' based on our model for the training set\n",
        "\n",
        "#Computed test set results too\n",
        "predictvals = m*p_mw + n*p_hydr + b\n",
        "\n",
        "#Do stats -- training set\n",
        "#Compute kendall tau and pvalue\n",
        "ktau, pvalue = scipy.stats.kendalltau( known_solubilities, fittedvals)\n",
        "#RMS error\n",
        "rms = tools.rmserr( known_solubilities, fittedvals)\n",
        "#Correlation coefficient\n",
        "R2 = tools.correl( known_solubilities, fittedvals)**2\n",
        "halflog = tools.percent_within_half( predict_solubilities, predictvals ) #Figure out percentage within 0.5 log units\n",
        "print(\"For mw+hydration model test, Kendall tau is %.2f, RMS error is %.2f, and Rsquared is %.2f. Probability of getting this Kendall tau value when in fact there is no correlation (null hypothesis): %.2g. Percentage within 0.5 log units: %.2f\" % (ktau, rms, R2, pvalue, halflog))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLa5aXoBlFiN"
      },
      "source": [
        "# Do your assignment below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFB4mb0_lFiN"
      },
      "outputs": [],
      "source": [
        "#============================================================================\n",
        "#ADD YOUR MODELS HERE, FOLLOWING THE PATTERNS OF THE TWO SIMPLE MODELS ABOVE\n",
        "#============================================================================"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda root]",
      "language": "python",
      "name": "conda-root-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "toc": {
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "name": "physprops_solubility.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}